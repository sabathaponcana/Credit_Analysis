{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = pd.read_csv('loan_data_2007_2014.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display ALL the colunms in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emp_length'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove the strings in the number of years in employment term to int, we create a new column\n",
    "\n",
    "df['employed_length'] = df['emp_length'].str.replace('years', '')\n",
    "df['employed_length'] = df['employed_length'].str.replace('< 1 year', str(0))\n",
    "df['employed_length'] = df['employed_length'].str.replace('nan', str(0))\n",
    "df['employed_length'] = df['employed_length'].str.replace('+', '')\n",
    "df['employed_length'] = df['employed_length'].str.replace(' year', '')\n",
    "df['employed_length'] = df['employed_length'].str.replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['employed_length'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['employed_length'] = pd.to_numeric(df['employed_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['employed_length'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['employed_length'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['term'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['loan_term'] = df['term'].str.replace(' ', '')\n",
    "df['loan_term'] = df['loan_term'].str.replace('months', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['loan_term'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['loan_term'] = pd.to_numeric(df['loan_term'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['loan_term'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['issue_d'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['issue_date'] = lambda i: dt.datetime(int(i[5:9]),int(i[9:11])).strftime('%b-%y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To change dates to datetime\n",
    "\n",
    "df['issue_date'] = pd.to_datetime(df['issue_d'], format = '%b-%y')\n",
    "df['earliest_credit_line_date'] = pd.to_datetime(df['earliest_cr_line'], format = '%b-%y')\n",
    "df['last_payment_date'] = pd.to_datetime(df['last_pymnt_d'], format = '%b-%y')\n",
    "df['last_credit_pull_date'] = pd.to_datetime(df['last_credit_pull_d'], format = '%b-%y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['issue_date'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the TIME since the earliest credit line was issued to use it for regression. Since we have the \n",
    "# earliest_credit_line_date' we can use the current date to determine that. But for this data we'll use 2017-12-01\n",
    "\n",
    "pd.to_datetime('2017-12-01') - df['issue_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert it to MONTHS \n",
    "    \n",
    "df['months_since_earliest_cr_line'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - df['issue_date'])\n",
    "                                                         / np.timedelta64(1, 'M')))\n",
    "# np.timedelta64(1, 'M') this expression converts time difference in days to months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['months_since_earliest_cr_line'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we had negative min of months, this how we would solve it:\n",
    "\n",
    "# 1. Explore the observations we calculated from our 'months_since_earliest_cr_line' where the number is negative \n",
    "# df.loc[: , ['earliest_cr_line','earliest_credit_line_date','months_since_earliest_cr_line']]['months_since_earliest_cr_line'] < 0]\n",
    "\n",
    "# 2. We assign these values with the MAX of the column\n",
    "# df['months_since_earliest_cr_line'][df['months_since_earliest_cr_line'] < 0] = df['months_since_earliest_cr_line'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Dummy variables for Discrete variables\n",
    "\n",
    "pd.get_dummies(df['grade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df['grade'], prefix = 'grade', prefix_sep = ':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create a new dataframe for the Dummy variables of all the categorical columns\n",
    "\n",
    "df_dummies = [pd.get_dummies(df['grade'], prefix = 'grade', prefix_sep = ':'),\n",
    "              pd.get_dummies(df['sub_grade'], prefix = 'sub_grade', prefix_sep = ':'),\n",
    "              pd.get_dummies(df['home_ownership'], prefix = 'home_ownership', prefix_sep = ':'),\n",
    "              pd.get_dummies(df['verification_status'], prefix = 'graverification_statusde', prefix_sep = ':'),\n",
    "              pd.get_dummies(df['loan_status'], prefix = 'loan_status', prefix_sep = ':'),\n",
    "              pd.get_dummies(df['purpose'], prefix = 'purpose', prefix_sep = ':'),\n",
    "              pd.get_dummies(df['addr_state'], prefix = 'addr_state', prefix_sep = ':'),\n",
    "              pd.get_dummies(df['initial_list_status'], prefix = 'initial_list_status', prefix_sep = ':')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make a dataframe we concat with pd.dataframe\n",
    "\n",
    "df_dummies = pd.concat(df_dummies, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, df_dummies], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values and  clean\n",
    "#pd.options.display.max_rows = None\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_rev_hi_lim'].fillna(df['funded_amnt'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['annual_inc'].fillna(df['annual_inc'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['delinq_2yrs'].fillna(0, inplace = True)\n",
    "df['inq_last_6mths'].fillna(0, inplace = True)\n",
    "df['open_acc'].fillna(0, inplace = True)\n",
    "df['pub_rec'].fillna(0, inplace = True)\n",
    "df['total_acc'].fillna(0, inplace = True)\n",
    "df['acc_now_delinq'].fillna(0, inplace = True)\n",
    "df['total_rev_hi_lim'].fillna(0, inplace = True)\n",
    "df['employed_length'].fillna(0, inplace = True)\n",
    "df['months_since_earliest_cr_line'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PD MODEL DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['loan_status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many accounts are there for each status\n",
    "\n",
    "df['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the proportion of accounts by status - we divide the value_count expression with the total count\n",
    "\n",
    "df['loan_status'].value_counts() / df['loan_status'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more precise Default definition(Good-Bad) with dummy variables\n",
    "\n",
    "df['good_bad'] = np.where(df['loan_status'].isin(['Charged Off','Default',\n",
    "                                                  'Does not meet the credit policy. Status:Charged Off',\n",
    "                                                  'Late (31-120 days)']), 0, 1) # Theses are all bad or defaulted accounts\n",
    "                                                                                # 0 = True - the loan has a default status\n",
    "                                                                                # 1 = False - the loan has no default status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['good_bad']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our input and outputs comes from Df dataframe. Our input is everything in the DF dataframe and output is the 'good_bad' column\n",
    "# In train_test_split(input, out)\n",
    "\n",
    "#train_test_split(df.drop('good_bad', axis = 1), df['good_bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inputs_train,df_inputs_test,df_targets_train,df_targets_test = train_test_split(df.drop('good_bad', axis = 1), \n",
    "                                                                                   df['good_bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inputs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inputs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_targets_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_targets_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inputs_train,df_inputs_test,df_targets_train,df_targets_test = train_test_split(df.drop('good_bad', axis = 1), \n",
    "                                                                                   df['good_bad'], test_size = 0.2,\n",
    "                                                                                   random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inputs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inputs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_targets_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_targets_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a dataframe for all the pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_prep = df_inputs_train # independents variables\n",
    "df_targets_prep = df_targets_train # dependents variables\n",
    "\n",
    "#df_input_prep = df_inputs_test\n",
    "#df_targets_prep = df_targets_test\n",
    "\n",
    "#####\n",
    "#df_inputs_prepr = loan_data_inputs_train\n",
    "#df_targets_prepr = loan_data_targets_train\n",
    "#####\n",
    "#df_inputs_prepr = loan_data_inputs_test\n",
    "#df_targets_prepr = loan_data_targets_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. To assess the explanatory power all the inputs columns with respect to the outcome of interest that is being a good or bad borrower. We going to use 'grade' column for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for unique values using our pre-pro df\n",
    "\n",
    "df_input_prep['grade'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe to store only the independent variable 'grade' from df_inputs_prep and the dependent 'good_bad' from\n",
    "# df_targets_prep dataframe\n",
    "\n",
    "df1  = pd.concat([df_input_prep['grade'], df_targets_prep], axis = 1)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. To calculate the Weight of Evidence for this variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of borrowers for each grade, we going to group 'grade'- we going to do that by indexing the columns because\n",
    "# want the df to be reusable.\n",
    "\n",
    "df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need the proportion of good borrowers and bad borrowers are within each group. This can be summerized either by proportion \n",
    "# of good borrowers or bad borrowers, it doesn't matter which one, since proportion of good borrowers = 1 - proportion of bad\n",
    "# borrowers. To calculate the prop of good borrowers,we going to use the average of good_bad\n",
    "\n",
    "df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now merge the aboves results into one dataframe\n",
    "\n",
    "df1 = pd.concat([df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].count(),\n",
    "                df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].mean()], axis = 1)\n",
    "    \n",
    "df1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop one the 'grade' column because they have the same values\n",
    "\n",
    "df1 = df1.iloc[ : ,[0,1,3]]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The columns names need to be more meaningful, so change df1[1] to observations and df1[2] to proportion of good borrowers\n",
    "\n",
    "df1.columns = [df1.columns.values[0], 'NumOfObser', 'PropOfGood']\n",
    "df1             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We going to use these two columns for calculations of WoE and Information Value.\n",
    "# To calculate the Propr of Observation that falls in each grade\n",
    "\n",
    "df1['PropObs'] = df1['NumOfObser'] / df1['NumOfObser'].sum()\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of good borrowers and number of bad borrowers by grade group. \n",
    "# good_borrowers = prop_good * totalNumberOfObser in respective to the grade\n",
    "\n",
    "df1['NumGoodBorrowers'] = df1['PropOfGood'] * df1['NumOfObser']\n",
    "df1['NumBadBorrowers'] = (1 - df1['PropOfGood']) * df1['NumOfObser']\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We calculate the proportion of good borrowers and bad borrowers of each grade.\n",
    "# the percentage of good borrowers in respective grade category = number of good in respective category / totalGoodBorrowers\n",
    "\n",
    "df1['PercntcOfGoodPerGrade'] = df1['NumGoodBorrowers'] / df1['NumGoodBorrowers'].sum()\n",
    "df1['PercntOfBadPerGrade'] = df1['NumBadBorrowers'] / df1['NumBadBorrowers'].sum()\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the WoE for the grade column: WoE = log(prop of good obs / prop of bad)\n",
    "\n",
    "df1['WoE'] = np.log(df1['PercntcOfGoodPerGrade'] / df1['PercntOfBadPerGrade'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is useful to sort the data by WoE to see which category has the highest default rate\n",
    "\n",
    "df1 = df1.sort_values(['WoE'])\n",
    "df1 = df1.reset_index(drop = True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the Information Value(IV).\n",
    "# IV = ((Proportion of good borrowers - Proportion of bad borrowers) * WoE).sum()\n",
    "# \n",
    "\n",
    "df1['IV'] = (df1['PercntcOfGoodPerGrade'] - df1['PercntOfBadPerGrade']) * df1['WoE']\n",
    "df1['IV'] = df1['IV'].sum()\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To automate for other discrete variables or columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to perform all calculations for any given pair of categorical variables and dependent variables\n",
    "# df - for our independent variables dataframe\n",
    "\n",
    "def WoE_discrete(df, discrete_variable_name, dependent_variable_name):\n",
    "    #1. concat the two variables from processed df\n",
    "    df = pd.concat([df[discrete_variable_name], dependent_variable_name], axis = 1) \n",
    "    \n",
    "    #2. Calculate the number of observations and proportion of good borrowers for each discrete variable\n",
    "    df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),\n",
    "                   df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)\n",
    "    \n",
    "    #3. Copy all subsequential expressions\n",
    "    df = df.iloc[ : ,[0,1,3]]\n",
    "    df.columns = [df.columns.values[0], 'NumOfObser', 'PropOfGood']\n",
    "    df['PropObs'] = df['NumOfObser'] / df['NumOfObser'].sum()\n",
    "    df['NumGoodBorrowers'] = df['PropOfGood'] * df['NumOfObser']\n",
    "    df['NumBadBorrowers'] = (1 - df['PropOfGood']) * df['NumOfObser']\n",
    "    df['PercntcOfGoodPerGrade'] = df['NumGoodBorrowers'] / df['NumGoodBorrowers'].sum()\n",
    "    df['PercntOfBadPerGrade'] = df['NumBadBorrowers'] / df['NumBadBorrowers'].sum()\n",
    "    df['WoE'] = np.log(df['PercntcOfGoodPerGrade'] / df['PercntOfBadPerGrade'])\n",
    "    df = df.sort_values(['WoE'])\n",
    "    df = df.reset_index(drop = True)\n",
    "    df['IV'] = (df['PercntcOfGoodPerGrade'] - df['PercntOfBadPerGrade']) * df['WoE']\n",
    "    df['IV'] = df['IV'].sum()\n",
    "    \n",
    "    #4. Return the df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WoE = WoE_discrete(df_input_prep, 'grade', df_targets_prep)\n",
    "df_WoE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing discrete variables: Visualizing Resulting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to plot by WoE dataframe\n",
    "\n",
    "def df_WoE_plot(df_WoE, rotation_on_Xaxis_label = 0):\n",
    "    x = np.array(df_WoE.iloc[ : , 0].apply(str)) # for the x_axis with any discrete column on the first position\n",
    "    y = df_WoE['WoE'] # the WoE column of every categorical or discrete variable\n",
    "    plt.figure(figsize = (18, 6))\n",
    "    plt.plot(x, y, marker = 'o', linestyle = '--', color = 'k')\n",
    "    plt.xlabel(df_WoE.columns[0])\n",
    "    plt.ylabel('Weight of Evidence')\n",
    "    plt.title('Weight of Evidence by ' + df_WoE.columns[0])\n",
    "    plt.xticks(rotation = rotation_on_Xaxis_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WoE_plot(df_WoE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WoE = WoE_discrete(df_input_prep, 'home_ownership', df_targets_prep)\n",
    "df_WoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WoE_plot(df_WoE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing discrete variables create dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take values of 'Home_owner' column to make dummies for the low WoE values into one column of dummies ie. \n",
    "#Rent, None and Other with Rent as they are the risk categories.\n",
    "\n",
    "df_input_prep['home_ownership:Rent_Other_None_Any'] = sum([df_input_prep['home_ownership:RENT'],\n",
    "                                                            df_input_prep['home_ownership:OTHER'],\n",
    "                                                            df_input_prep['home_ownership:ANY'],\n",
    "                                                            df_input_prep['home_ownership:NONE']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More advance Preprocessing for PD Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this we will look at addr_state variable\n",
    "\n",
    "df_input_prep['addr_state'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_WoE_addr_state = WoE_discrete(df_input_prep, 'addr_state', df_targets_prep)\n",
    "df_WoE_addr_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WoE_plot(df_WoE_addr_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see that there is only 50 values or states, that's because one is missing since there's no borrowers from there ie North\n",
    "# Dakota(ND) so we doing to add a dummy variable of that state for future reference if it happens to have borrowers from there.\n",
    "\n",
    "if ['add_state:ND'] in df_input_prep.columns.values:\n",
    "    pass\n",
    "else:\n",
    "    df_input_prep['add_state:ND'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since NE, IA, ME and ID have high WoE because of low observations we will put them into High risk and Low risk respectively.\n",
    "# Plot the without these 4 states\n",
    "\n",
    "df_WoE_plot(df_WoE_addr_state.iloc[2 : -2, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot we can see that NV,FL,HI,AL,NE,IA and ND can be put in one category, we can also see that the last 6 states\n",
    "with low risk from WV to DC including the 2 states we left out ME and ID can be put in one category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now can we look at other 38 states\n",
    "\n",
    "df_WoE_plot(df_WoE_addr_state.iloc[6 : -6, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows that NM to CA have similar WoE score but when looking the number of observations, NY and CA have a higher number.\n",
    "Thus CA and NY will have their own categories, then NM and VA, OK to NC will have seperate categories. Even though some areas may have the same WoE score but it is important to take inconsideration the number of observation so if a state has a high number of observation it's best to put those in a single group category and the ones with 2 or more in a separate category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create the dummy variables for those with 2 or more states\n",
    "\n",
    "df_input_prep['addr_state:NV_FL_HI_AL_NE_IA_ND'] = sum([df_input_prep['addr_state:NV'], df_input_prep['addr_state:FL'],\n",
    "                                                       df_input_prep['addr_state:HI'],df_input_prep['addr_state:AL'],\n",
    "                                                       df_input_prep['addr_state:NE'],df_input_prep['addr_state:IA']])\n",
    "\n",
    "df_input_prep['addr_state:NM_VA'] = sum([df_input_prep['addr_state:NM'],df_input_prep['addr_state:VA']])\n",
    "\n",
    "df_input_prep['addr_state:OK_TN_MO_LA_MD_NC'] = sum([df_input_prep['addr_state:NC'],df_input_prep['addr_state:MD'],\n",
    "                                                    df_input_prep['addr_state:LA'],df_input_prep['addr_state:MO'],\n",
    "                                                    df_input_prep['addr_state:TN'],df_input_prep['addr_state:OK']])\n",
    "\n",
    "df_input_prep['addr_state:UT_KY_AZ_NJ'] = sum([df_input_prep['addr_state:UT'],df_input_prep['addr_state:KY'],\n",
    "                                              df_input_prep['addr_state:AZ'],df_input_prep['addr_state:NJ']])\n",
    "\n",
    "df_input_prep['addr_state:AR_MI_PA_OH_MN'] = sum([df_input_prep['addr_state:AR'],df_input_prep['addr_state:MI'],\n",
    "                                                 df_input_prep['addr_state:PA'],df_input_prep['addr_state:OH'],\n",
    "                                                 df_input_prep['addr_state:MN']])\n",
    "\n",
    "df_input_prep['addr_state:RI_MA_DE_SD_IN'] = sum([df_input_prep['addr_state:RI'],df_input_prep['addr_state:MA'],\n",
    "                                                 df_input_prep['addr_state:DE'],df_input_prep['addr_state:SD'],\n",
    "                                                 df_input_prep['addr_state:IN']])\n",
    "\n",
    "df_input_prep['addr_state:GA_WA_OR'] = sum([df_input_prep['addr_state:GA'],df_input_prep['addr_state:WA'],\n",
    "                                           df_input_prep['addr_state:OR']])\n",
    "\n",
    "df_input_prep['addr_state:WI_MT'] = sum([df_input_prep['addr_state:WI'],df_input_prep['addr_state:MT']])\n",
    "\n",
    "df_input_prep['addr_state:IL_CT'] = sum([df_input_prep['addr_state:IL'],df_input_prep['addr_state:CT']])\n",
    "\n",
    "df_input_prep['addr_state:KS_SC_CO_VT_AK_MS'] = sum([df_input_prep['addr_state:KS'],df_input_prep['addr_state:SC'],\n",
    "                                                    df_input_prep['addr_state:VT'],df_input_prep['addr_state:AK'],\n",
    "                                                    df_input_prep['addr_state:MS']])\n",
    "\n",
    "df_input_prep['addr_state:WV_NH_WY_DC_ME_ID'] = sum([df_input_prep['addr_state:WV'],df_input_prep['addr_state:NH'],\n",
    "                                                    df_input_prep['addr_state:WY'],df_input_prep['addr_state:DC'],\n",
    "                                                    df_input_prep['addr_state:ME'],df_input_prep['addr_state:ID']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of Continuous variables Automating calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same function for calculating WoE for Discrete variabls but changing name of function and sort and reset.\n",
    "\n",
    "\n",
    "def WoE_ordered_continuous(df, discrete_variable_name, dependent_variable_name):\n",
    "    #1. concat the two variables from processed df\n",
    "    df = pd.concat([df[discrete_variable_name], dependent_variable_name], axis = 1) \n",
    "    \n",
    "    #2. Calculate the number of observations and proportion of good borrowers for each discrete variable\n",
    "    df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),\n",
    "                   df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)\n",
    "    \n",
    "    #3. Copy all subsequential expressions\n",
    "    df = df.iloc[ : ,[0,1,3]]\n",
    "    df.columns = [df.columns.values[0], 'NumOfObser', 'PropOfGood']\n",
    "    df['PropObs'] = df['NumOfObser'] / df['NumOfObser'].sum()\n",
    "    df['NumGoodBorrowers'] = df['PropOfGood'] * df['NumOfObser']\n",
    "    df['NumBadBorrowers'] = (1 - df['PropOfGood']) * df['NumOfObser']\n",
    "    df['PercntcOfGoodPerGrade'] = df['NumGoodBorrowers'] / df['NumGoodBorrowers'].sum()\n",
    "    df['PercntOfBadPerGrade'] = df['NumBadBorrowers'] / df['NumBadBorrowers'].sum()\n",
    "    df['WoE'] = np.log(df['PercntcOfGoodPerGrade'] / df['PercntOfBadPerGrade'])\n",
    "    #df = df.sort_values(['WoE'])\n",
    "    #df = df.reset_index(drop = True)\n",
    "    df['DiffOfProprGood'] = df['PropOfGood'].diff().abs()\n",
    "    df['DiffInWoE'] = df['WoE'].diff().abs()\n",
    "    df['IV'] = (df['PercntcOfGoodPerGrade'] - df['PercntOfBadPerGrade']) * df['WoE']\n",
    "    df['IV'] = df['IV'].sum()\n",
    "    \n",
    "    #4. Return the df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of continuous creating dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the 'loan_term' variable\n",
    "\n",
    "df_input_prep['loan_term'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the WoE for this variable using the WoE_continuous function\n",
    "\n",
    "df_loan_term_WoE = WoE_ordered_continuous(df_input_prep, 'loan_term', df_targets_prep)\n",
    "df_loan_term_WoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the df\n",
    "\n",
    "df_WoE_plot(df_loan_term_WoE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot we can see that 60 months loans are riskier than 36 months, so we create dummy variables for both categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the 36 months variable\n",
    "# np.where(condition, if True, else if False)\n",
    "\n",
    "df_input_prep['term:36'] = np.where((df_input_prep['loan_term'] == 36),1 ,0)\n",
    "df_input_prep['term:60'] = np.where((df_input_prep['loan_term'] == 60),1 ,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the employment term variable\n",
    "\n",
    "df_input_prep['employed_length'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_empl_length_WoE = WoE_ordered_continuous(df_input_prep, 'employed_length', df_targets_prep)\n",
    "df_empl_length_WoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WoE_plot(df_empl_length_WoE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, we can categorize the variables as thus: 0, 1, 2 to 4, 5 to 6, 7 to 9 and 10 over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we going to use the isin function to pass a list of options and a range function\n",
    "\n",
    "df_input_prep['employment:0'] = np.where(df_input_prep['employed_length'].isin([0]), 1, 0)\n",
    "df_input_prep['employment:1'] = np.where(df_input_prep['employed_length'].isin([1]), 1, 0)\n",
    "df_input_prep['employment:2-4'] = np.where(df_input_prep['employed_length'].isin(range(2 , 5)), 1, 0)\n",
    "df_input_prep['employment:5-6'] = np.where(df_input_prep['employed_length'].isin(range(5, 7)), 1, 0)\n",
    "df_input_prep['employment:7-9'] = np.where(df_input_prep['employed_length'].isin(range(7, 10)), 1, 0)\n",
    "df_input_prep['employment:10+'] = np.where(df_input_prep['employed_length'].isin([10]), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we going to work with a more complex one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_prep['months_since_earliest_cr_line'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have about 90 values,we can still work with them without any complications. But it is best to work with 50 or less\n",
    "# values. So we going to use Pandas Cut() method to categorise the values. Cut(Series/column, number of categories)\n",
    "\n",
    "df_input_prep['months_since_issue_date_factor'] = pd.cut(df_input_prep['months_since_earliest_cr_line'], 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now we see that with every row there's an interval\n",
    "\n",
    "df_input_prep['months_since_issue_date_factor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the WoE for the new variable\n",
    "\n",
    "df_Months_issue_date_WoE = WoE_ordered_continuous(df_input_prep,'months_since_issue_date_factor', df_targets_prep)\n",
    "df_Months_issue_date_WoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WoE_plot(df_Months_issue_date_WoE, 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot showcase that the first three categories differ from each other in terms of WoE and the next category, thus we will keep them for final\n",
    "model. So we will plot from 4th category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WoE_plot(df_Months_issue_date_WoE.iloc[3: ,: ], 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot we can make categories as follows: 42 - 48, 49 - 52, 53 - 64. From 65 onwards we see that the plot starts\n",
    "to make a up and down spiral, from this we have look at the number of observation for the plots. We notice that number of observations\n",
    "don't differ that much and there's no hight or spike in them therefore we can create one category from 65 onwards or we can make\n",
    "2 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_prep['months_issue_date:<38'] = np.where(df_input_prep['months_since_earliest_cr_line'].isin(range(38)),1 ,0)\n",
    "df_input_prep['months_issue_date:<38'] = np.where(df_input_prep['months_since_earliest_cr_line'].isin(range(38, 40)),1 ,0)\n",
    "df_input_prep['months_issue_date:<38'] = np.where(df_input_prep['months_since_earliest_cr_line'].isin(range(40, 42)),1 ,0)\n",
    "df_input_prep['months_issue_date:<38'] = np.where(df_input_prep['months_since_earliest_cr_line'].isin(range(42, 49)),1 ,0)\n",
    "df_input_prep['months_issue_date:<38'] = np.where(df_input_prep['months_since_earliest_cr_line'].isin(range(49, 53)),1 ,0)\n",
    "df_input_prep['months_issue_date:<38'] = np.where(df_input_prep['months_since_earliest_cr_line'].isin(range(53, 65)),1 ,0)\n",
    "df_input_prep['months_issue_date:<38'] = np.where(df_input_prep['months_since_earliest_cr_line'].isin(range(65, 85)),1 ,0)\n",
    "df_input_prep['months_issue_date:<38'] = np.where(df_input_prep['months_since_earliest_cr_line'].isin(range(85, \n",
    "                                                            int(df_input_prep['months_since_earliest_cr_line'].max()))),1 ,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do the same for interest rate\n",
    "\n",
    "df_input_prep['int_rate'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_prep['interest_rate_factor'] = pd.cut(df_input_prep['int_rate'], 50)\n",
    "df_input_prep['interest_rate_factor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Interest_rate = WoE_ordered_continuous(df_input_prep, 'interest_rate_factor', df_targets_prep)\n",
    "df_Interest_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WoE_plot(df_Interest_rate, 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows that the relationship between WoE and Interest rate is almost monotonic, that means, the greater the Interest\n",
    "rate the lower the WoE and the higher the Probability of default. The presence of monotonic relationship makes it easier to\n",
    "determine the boundaries of the intervals.\n",
    "\n",
    "The first 10 points shows an up and down intervals, this cause us to look at the number of observations. We can conclude that \n",
    "there's no big difference in number of observations between them therefore we can make them to be one category, the cut-off\n",
    "of this will be between 10th and 11th category is 9.548. The following categories will be thus: 9.548 >= 12.025, 12.025 >= 15.740,\n",
    "15.740 >= 20.281 and greater than 20.281    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_prep['interest:<9.548'] = np.where((df_input_prep['int_rate'] <= 9.548),1 ,0)\n",
    "df_input_prep['interest:9.548-12.025'] = np.where((df_input_prep['int_rate'] > 9.548) & (df_input_prep['int_rate'] <= 12.025)\n",
    "                                                  ,1 ,0)\n",
    "df_input_prep['interest:12.025-15.740'] = np.where((df_input_prep['int_rate'] > 12.025) & (df_input_prep['int_rate'] <= 15.740)\n",
    "                                                  ,1 ,0)\n",
    "df_input_prep['interest:15.740-20.281'] = np.where((df_input_prep['int_rate'] > 15.740) & (df_input_prep['int_rate'] <= 20.281)\n",
    "                                                  ,1 ,0)\n",
    "df_input_prep['interest:9.548-12.025'] = np.where((df_input_prep['int_rate'] > 20.281),1 ,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do the same for funded amount\n",
    "\n",
    "df_input_prep['funded_amnt'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_input_prep['funded_amnt_factor'] = pd.cut(df_input_prep['funded_amnt'], 50)\n",
    "df_FundedAmnt_WoE = WoE_ordered_continuous(df_input_prep, 'funded_amnt_factor', df_targets_prep)\n",
    "df_FundedAmnt_WoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WoE_plot(df_FundedAmnt_WoE, 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows that there's no relationship between the WoE and the independent variables. Thus we cannot use the amount \n",
    "funded variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do the same for annual income\n",
    "\n",
    "df_input_prep['annual_incm_factor'] = pd.cut(df_input_prep['annual_inc'], 50)\n",
    "df_Annual_incm_WoE = WoE_ordered_continuous(df_input_prep, 'annual_incm_factor', df_targets_prep)\n",
    "df_Annual_incm_WoE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows that the 1st category has more that 90% of all our observations thus we can try to increase the categories \n",
    "to cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's increase the number of categories to cut\n",
    "\n",
    "df_input_prep['annual_incm_factor'] = pd.cut(df_input_prep['annual_inc'], 100)\n",
    "df_Annual_incm_WoE = WoE_ordered_continuous(df_input_prep, 'annual_incm_factor', df_targets_prep)\n",
    "df_Annual_incm_WoE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we can see the above plot shows a large number of observations in the first category. It will be best to put people with\n",
    "high income first than investigate people with low income. The 1st 2 categories contains alot of observations and the number\n",
    "of observations decrease as the annual income increase. We going to creat a category for people with high income now than see\n",
    "how we can categorise the low income. So we will at the threshold of people earning lower than 140,000.00 as low income and \n",
    "140,000.00+ as higher income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a df for low income\n",
    "\n",
    "df_input_prep_low_incm = df_input_prep.loc[df_input_prep['annual_inc'] <= 140000.00, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the WoE for new df\n",
    "\n",
    "df_input_prep_low_incm['annual_incm_factor'] = pd.cut(df_input_prep_low_incm['annual_inc'], 50)\n",
    "df_input_prep_low_incm['annual_incm_factor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Annual_income = WoE_ordered_continuous(df_input_prep_low_incm, 'annual_incm_factor', df_targets_prep[df_targets_prep.index])\n",
    "df_Annual_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WoE_plot(df_Annual_income, 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see above is that WoE increases monotonically with income. So our categories will be thus: 0-20, then from 20 - 100 we \n",
    "will have equal 10k intervals, 100 - 120, 120 - 140 then 140+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_prep['annaul_incm:<20k'] = np.where((df_input_prep['annual_inc'] <= 20000), 1, 0)\n",
    "df_input_prep['annual_inmcm:20k-30k'] = np.where((df_input_prep['annual_inc'] > 20000 & (df_input_prep['annual_inc'] <= 30000)),\n",
    "                                                1, 0)\n",
    "df_input_prep['annual_inmcm:30k-40k'] = np.where((df_input_prep['annual_inc'] > 30000 & (df_input_prep['annual_inc'] <= 40000)),\n",
    "                                                1, 0)\n",
    "df_input_prep['annual_inmcm:40k-50k'] = np.where((df_input_prep['annual_inc'] > 40000 & (df_input_prep['annual_inc'] <= 50000)),\n",
    "                                                1, 0)\n",
    "df_input_prep['annual_inmcm:50k-60k'] = np.where((df_input_prep['annual_inc'] > 50000 & (df_input_prep['annual_inc'] <= 60000)),\n",
    "                                                1, 0)\n",
    "df_input_prep['annual_inmcm:70k-80k'] = np.where((df_input_prep['annual_inc'] > 70000 & (df_input_prep['annual_inc'] <= 80000)),\n",
    "                                                1, 0)\n",
    "df_input_prep['annual_inmcm:80k-90k'] = np.where((df_input_prep['annual_inc'] > 80000 & (df_input_prep['annual_inc'] <= 90000)),\n",
    "                                                1, 0)\n",
    "df_input_prep['annual_inmcm:90k-100k'] = np.where((df_input_prep['annual_inc'] > 90000&(df_input_prep['annual_inc'] <= 100000)),\n",
    "                                                1, 0)\n",
    "df_input_prep['annual_inmcm:100k-120k'] = np.where((df_input_prep['annual_inc'] > 100000 & (df_input_prep['annual_inc'] <= 120000)),\n",
    "                                                1, 0)\n",
    "df_input_prep['annual_inmcm:120k-140k'] = np.where((df_input_prep['annual_inc'] > 120000 & (df_input_prep['annual_inc'] <= 140000)),\n",
    "                                                1, 0)\n",
    "df_input_prep['annual_inmcm:>140k'] = np.where((df_input_prep['annual_inc'] > 140000),1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's do it for 'months since last deliquency'\n",
    "\n",
    "df_months_last_delinq = df_input_prep[pd.notnull(df_input_prep['mths_since_last_delinq'])]\n",
    "df_months_last_delinq['months_last_deliq_factor'] = pd.cut(df_months_last_delinq['mths_since_last_delinq'], 50)\n",
    "df_Delinq_WoE = WoE_ordered_continuous(df_months_last_delinq, 'months_last_deliq_factor', \n",
    "                                       df_targets_prep[df_months_last_delinq.index])\n",
    "df_Delinq_WoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WoE_plot(df_Delinq_WoE, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_prep['mnths_last_last_delinq:missing'] = np.where((df_input_prep['mths_since_last_delinq'].isnull()),1 ,0)\n",
    "df_input_prep['mnths_last_last_delinq:0-3'] = np.where((df_input_prep['mths_since_last_delinq'] >= 0) & (df_input_prep['mths_since_last_delinq'] <= 3)\n",
    "                                                       ,1, 0)\n",
    "df_input_prep['mnths_last_last_delinq:4-30'] = np.where((df_input_prep['mths_since_last_delinq'] >= 4) & (df_input_prep['mths_since_last_delinq'] <= 30),\n",
    "                                                      1, 0)\n",
    "df_input_prep['mnths_last_last_delinq:31-56'] = np.where((df_input_prep['mths_since_last_delinq'] >= 31) & (df_input_prep['mths_since_last_delinq'] <= 56),\n",
    "                                                      1, 0)\n",
    "df_input_prep['mnths_last_last_delinq:>56'] = np.where((df_input_prep['mths_since_last_delinq'] >= 57), 1, 0)                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we used this code 'df_input_prep = df_inputs_train and df_targets_prep = df_targets_train'to get the dummy\n",
    "variables. The 'df_input_prep' contained the old data set with new dummy variables, so that means we can use the same code\n",
    "to create the same dummy variables for the test dats set. We don't have to calculate the WoE or come up with dummy variable for test data since test data set \n",
    "is only used to test and assess a model. Before we use the same code, we have to save the train data in a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the 'df_input_prep'\n",
    "\n",
    "loan_data_inputs_train = df_input_prep\n",
    "#loan_data_inputs_test = df_input_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save everything  to csv file\n",
    "\n",
    "#loan_data_inputs_train.to_csv('loan_data_inputs_train.csv')\n",
    "#df_targets_train.to_csv('df_targets_train.csv')\n",
    "#loan_data_inputs_test.to_csv('loan_data_inputs_test.csv')\n",
    "#df_targets_test.to_csv('df_targets_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
